{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance of Machine Learning Algorithms with Resampling\n",
    "\n",
    "\n",
    "><small><i>from the book \n",
    "\"Machine Learning Mastery With Python: Understand Your Data, Create Accurate Models and Work Projects End-To-End\"\n",
    "by Jason Brownlee, Migrated to Jupyter with additions by Mitch Sanders 2017</i></small>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to know how well your algorithms perform on unseen data. The best way to evaluate\n",
    "the performance of an algorithm would be to make predictions for new data to which you\n",
    "already know the answers. \n",
    "\n",
    "The second best way is to use clever techniques from statistics called\n",
    "**resampling methods** that allow you to make accurate estimates for how well your algorithm will\n",
    "perform on new data. In this section you will discover how you can estimate the accuracy of\n",
    "your machine learning algorithms using resampling methods in Python and scikit-learn on the\n",
    "Pima Indians dataset. \n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "## Evaluate Machine Learning Algorithms\n",
    "Why can’t you prepare your machine learning algorithm on your training dataset and use\n",
    "predictions from this same dataset to evaluate performance? The simple answer is overfitting.\n",
    "\n",
    "\n",
    "Imagine an algorithm that remembers every observation it is shown during training. If you\n",
    "evaluated your machine learning algorithm on the same dataset used to train the algorithm, then\n",
    "an algorithm like this would have a perfect score on the training dataset. But the predictions it\n",
    "made on new data would be terrible. We must evaluate our machine learning algorithms on\n",
    "data that is not used to train the algorithm.\n",
    "\n",
    "\n",
    "The evaluation is an *estimate* that we can use to talk about how well we think the algorithm\n",
    "may actually do in practice. It is not a guarantee of performance. Once we estimate the\n",
    "performance of our algorithm, **we can then re-train the final algorithm on the entire training\n",
    "dataset** and get it ready for operational use. \n",
    "\n",
    "Next up we are going to look at four different\n",
    "techniques that we can use to split up our training dataset and create useful estimates of\n",
    "performance for our machine learning algorithms:\n",
    "- Train and Test Sets.\n",
    "- k-fold Cross-Validation.\n",
    "- Leave One Out Cross-Validation.\n",
    "- Repeated Random Test-Train Splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split into Train and Test Sets\n",
    "The simplest method that we can use to evaluate the performance of a machine learning\n",
    "algorithm is to use different training and testing datasets. We can take our original dataset and\n",
    "split it into two parts. Train the algorithm on the first part, make predictions on the second\n",
    "part and evaluate the predictions against the expected results. The size of the split can depend\n",
    "on the size and specifics of your dataset, although it is common to use 67% of the data for\n",
    "training and the remaining 33% for testing.\n",
    "\n",
    "\n",
    "This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of\n",
    "records) where there is strong evidence that both splits of the data are representative of the\n",
    "underlying problem. Because of the speed, it is useful to use this approach when the algorithm\n",
    "you are investigating is slow to train. A downside of this technique is that it can have a high\n",
    "variance. This means that differences in the training and test dataset can result in meaningful\n",
    "differences in the estimate of accuracy. In the example below we split the Pima Indians dataset\n",
    "into 67%/33% splits for training and test and evaluate the accuracy of a Logistic Regression\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... quick learning detour... keeping Python modules working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## oops we have a problem.... Googling says our ski-kit learn version is old\n",
    "## this can get a bit complicated, particularly on a Windows with multiple\n",
    "## python environments: https://github.com/scikit-learn/scikit-learn/issues/6161\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='error_train_test_split.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation says I need version 0.18... let's check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='error_train_test_split2.png'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## quick and dirty is to go to python anaconda install DOS location\n",
    "## and UPDATE (not INSTALL) the module needing it... for me this was this command\n",
    "## \"conda update scikit-learn\" - here and like this:\n",
    "## C:\\Users\\mitch_sanders\\AppData\\Local\\Continuum\\Anaconda2>conda update scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='error_train_test_split3.png'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## let's restart the Kernal (Jupyter: Kernel -> Restart and Clear Output) \n",
    "## and import and check if new version\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... okay, back to the flow - Train and Test Set :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate using a train and a test set\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "my_test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=my_test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %%\") , (result*100.0)\n",
    "\n",
    "# for Python 3.x: 'Accuracy: {:4.3f}%'.format(result*100)\n",
    "# see https://pyformat.info/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that the estimated accuracy for the model was approximately 75%. Note that\n",
    "in addition to specifying the size of the split, we also specify the random seed. Because the\n",
    "split of the data is random, we want to ensure that the results are reproducible. By specifying\n",
    "the random seed we ensure that we get the same random numbers each time we run the code\n",
    "and in turn the same split of data. This is important if we want to compare this result to\n",
    "the estimated accuracy of another machine learning algorithm or the same algorithm with a\n",
    "different configuration. To ensure the comparison was apples-for-apples, we must ensure that\n",
    "they are trained and tested on exactly the same data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross-Validation\n",
    "Cross-validation is an approach that you can use to estimate the performance of a machine\n",
    "learning algorithm with less variance than a single train-test set split. It works by splitting\n",
    "the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The\n",
    "algorithm is trained on k − 1 folds with one held back and tested on the held back fold. This is\n",
    "repeated so that each fold of the dataset is given a chance to be the held back test set. After\n",
    "running cross-validation you end up with k different performance scores that you can summarize\n",
    "using a mean and a standard deviation.\n",
    "\n",
    "\n",
    "The result is a more reliable estimate of the performance of the algorithm on new data. It is\n",
    "more accurate because the algorithm is trained and evaluated multiple times on different data.\n",
    "The choice of k must allow the size of each test partition to be large enough to be a reasonable\n",
    "sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the\n",
    "algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest\n",
    "sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are\n",
    "common. In the example below we use 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate using Cross Validation\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %% (%%)\") , (results.mean()*100.0, results.std()*100.0)\n",
    "\n",
    "#print('Output of evaluating an algorithm with k-fold Cross-Validation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we report both the mean and the standard deviation of the performance\n",
    "measure. When summarizing performance measures, it is a good practice to summarize the\n",
    "distribution of the measures, in this case assuming a Gaussian distribution of performance (a\n",
    "very reasonable assumption) and recording the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross-Validation\n",
    "You can configure cross-validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). This variation of cross-validation is called leave-one-out crossvalidation. The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold cross-validation. In the example below we use leave-one-out cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate using Leave One Out Cross Validation\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "loocv = LeaveOneOut()\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy: %% (%%)\") , (results.mean()*100.0, results.std()*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the standard deviation that the score has more variance than the k-fold cross-validation results described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Repeated Random Test-Train Splits\n",
    "Another variation on k-fold cross-validation is to create a random split of the data like the\n",
    "train/test split described above, but repeat the process of splitting and evaluation of the\n",
    "algorithm multiple times, like cross-validation. This has the speed of using a train/test split and\n",
    "the reduction in variance in the estimated performance of k-fold cross-validation. You can also\n",
    "repeat the process many more times as needed to improve the accuracy. A down side is that\n",
    "repetitions may include much of the same data in the train or the test split from run to run,\n",
    "introducing redundancy into the evaluation. The example below splits the data into a 67%/33%\n",
    "train/test split and repeats the process 10 times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate using Shuffle Split Cross Validation\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "n_splits = 10\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %% (%%)\") , (results.mean()*100.0, results.std()*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in this case the distribution of the performance measure is on par with\n",
    "k-fold cross-validation above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Techniques to Use When\n",
    "This section lists some tips to consider what resampling technique to use in different circumstances.\n",
    "- Generally k-fold cross-validation is the gold standard for evaluating the performance of a\n",
    "machine learning algorithm on unseen data with k set to 3, 5, or 10.\n",
    "-  Using a train/test split is good for speed when using a slow algorithm and produces\n",
    "performance estimates with lower bias when using large datasets.\n",
    "- Techniques like leave-one-out cross-validation and repeated random splits can be useful\n",
    "intermediates when trying to balance variance in the estimated performance, model\n",
    "training speed and dataset size.\n",
    "\n",
    "The best advice is to experiment and find a technique for your problem that is fast and\n",
    "produces reasonable estimates of performance that you can use to make decisions. If in doubt,\n",
    "use 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Summary\n",
    "In this chapter you discovered statistical techniques that you can use to estimate the performance\n",
    "of your machine learning algorithms, called resampling. Specifically, you learned about:\n",
    "- Train and Test Sets.\n",
    "- Cross-Validation.\n",
    "- Leave One Out Cross-Validation.\n",
    "- Repeated Random Test-Train Splits.\n",
    "\n",
    "### Next\n",
    "In the next section you will learn how you can evaluate the performance of classification and\n",
    "regression algorithms using a suite of different metrics and built in evaluation reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Pima Indian Dataset \n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
